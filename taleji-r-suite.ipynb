{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ SMART MODE ACTIVE\n",
    "\n",
    "## üîç Automatic Data Detection\n",
    "\n",
    "This notebook now **automatically detects** available datasets in your Kaggle environment!\n",
    "\n",
    "### How it works:\n",
    "1. **üîç Auto-Discovery**: Scans `../input/` directory for competition datasets\n",
    "2. **üìä Smart Loading**: Automatically loads `train.csv` and `test.csv` from first dataset found\n",
    "3. **üéØ Column Detection**: Auto-detects target and ID columns using common patterns\n",
    "4. **üå∏ Fallback Mode**: Uses iris demo data if no competition data is found\n",
    "\n",
    "### Manual Override (Optional):\n",
    "If auto-detection doesn't work perfectly, you can manually set:\n",
    "\n",
    "```r\n",
    "# In cell 3, after auto-detection, override if needed:\n",
    "TARGET_COL <- \"your_actual_target_column\"\n",
    "ID_COL <- \"your_actual_id_column\"\n",
    "```\n",
    "\n",
    "### Supported Patterns:\n",
    "- **Target columns**: `\"target\"`, `\"label\"`, `\"y\"`, `\"survived\"`, `\"sale_price\"`, etc.\n",
    "- **ID columns**: `\"id\"`, `\"Id\"`, `\"ID\"`, `\"PassengerId\"`, `\"customer_id\"`, etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taleji R Suite: Complete Tidymodels Classification Workflow\n",
    "## üöÄ PRODUCTION MODE - Ready for Kaggle Competition\n",
    "\n",
    "This notebook demonstrates a comprehensive Random Forest classification pipeline using the **tidymodels** ecosystem. The workflow includes:\n",
    "\n",
    "- üîÑ **Kaggle data loading** (production mode active)\n",
    "- üéØ **Stratified train/validation splits** \n",
    "- ‚öôÔ∏è **Preprocessing pipeline** with imputation, encoding, and normalization\n",
    "- üîç **Hyperparameter tuning** with cross-validation\n",
    "- üìä **Model evaluation** with multiple metrics\n",
    "- üèÜ **Feature importance analysis**\n",
    "- üìù **Competition submission file generation**\n",
    "\n",
    "## üèÜ Production Setup\n",
    "\n",
    "‚úÖ **Kaggle data loading**: ACTIVE  \n",
    "‚úÖ **Competition submission**: ACTIVE  \n",
    "‚úÖ **Hyperparameter tuning**: ACTIVE  \n",
    "‚úÖ **Feature importance**: ACTIVE\n",
    "\n",
    "**Next**: Update file paths and column names in the setup cell above, then run all cells!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìã **Cell Execution Order**\n",
    "\n",
    "‚ö†Ô∏è **Important**: Run cells in order for proper functionality!\n",
    "\n",
    "| Cell | Description | Creates |\n",
    "|------|-------------|---------|\n",
    "| **1** | Setup Instructions | - |\n",
    "| **2** | Title & Overview | - |\n",
    "| **3** | Data Loading & Detection | `your_train_data_frame`, `test_data_processed` |\n",
    "| **4** | Data Split & Model Training | `train_data`, `val_data`, `your_recipe`, `final_model_fit` |\n",
    "| **5** | Hyperparameter Tuning | `final_tuned_fit`, `tuned_predictions` |\n",
    "| **6** | Feature Importance | `feature_importance`, plots |\n",
    "| **7** | Test Predictions & Submission | `test_predictions`, CSV files |\n",
    "| **8** | Advanced Techniques Guide | - |\n",
    "\n",
    "üí° **Tip**: If you get \"object not found\" errors, re-run the earlier cells that create those objects.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# QUICK START: Run All Cells Button Alternative\n",
    "# ==============================================================================\n",
    "# If you want to run the entire workflow at once, uncomment and run this cell\n",
    "\n",
    "# RUN_ALL_WORKFLOW <- TRUE\n",
    "# \n",
    "# if (exists(\"RUN_ALL_WORKFLOW\") && RUN_ALL_WORKFLOW) {\n",
    "#   cat(\"üöÄ Running complete workflow...\\n\\n\")\n",
    "#   \n",
    "#   # This would execute the entire pipeline programmatically\n",
    "#   # Uncomment the next line to enable:\n",
    "#   # source(\"complete_workflow.R\")  # If you save the workflow as a script\n",
    "#   \n",
    "#   cat(\"‚úÖ Workflow completed! Check the objects in your environment.\\n\")\n",
    "# } else {\n",
    "#   cat(\"üìù Quick start disabled. Run cells individually or uncomment RUN_ALL_WORKFLOW above.\\n\")\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SMART DATA LOADING (Auto-Detection + Fallback)\n",
    "# ==============================================================================\n",
    "# This section automatically detects available data paths or falls back to demo data\n",
    "\n",
    "library(readr)\n",
    "\n",
    "# Function to find available competition datasets\n",
    "find_competition_data <- function() {\n",
    "  # Check if we're in Kaggle environment\n",
    "  if (dir.exists(\"../input/\")) {\n",
    "    # List all available datasets in input directory\n",
    "    datasets <- list.dirs(\"../input/\", recursive = FALSE, full.names = FALSE)\n",
    "    cat(\"üìÅ Available datasets in ../input/:\\n\")\n",
    "    for (i in seq_along(datasets)) {\n",
    "      cat(sprintf(\"   %d. %s\\n\", i, datasets[i]))\n",
    "      # Check for common file patterns\n",
    "      dataset_path <- paste0(\"../input/\", datasets[i])\n",
    "      files <- list.files(dataset_path, pattern = \"\\\\.(csv|txt)$\", ignore.case = TRUE)\n",
    "      if (length(files) > 0) {\n",
    "        cat(sprintf(\"      Files: %s\\n\", paste(head(files, 3), collapse = \", \")))\n",
    "      }\n",
    "    }\n",
    "    return(datasets)\n",
    "  } else {\n",
    "    cat(\"üè† Not in Kaggle environment (../input/ not found)\\n\")\n",
    "    return(NULL)\n",
    "  }\n",
    "}\n",
    "\n",
    "# Auto-detect and load data\n",
    "datasets <- find_competition_data()\n",
    "\n",
    "# Try to load competition data automatically\n",
    "if (!is.null(datasets) && length(datasets) > 0) {\n",
    "  # Use the first dataset found (you can modify this logic)\n",
    "  competition_name <- datasets[1]\n",
    "  train_path <- paste0(\"../input/\", competition_name, \"/train.csv\")\n",
    "  test_path <- paste0(\"../input/\", competition_name, \"/test.csv\")\n",
    "  \n",
    "  cat(sprintf(\"üîç Attempting to load: %s\\n\", competition_name))\n",
    "  cat(sprintf(\"   Train: %s\\n\", train_path))\n",
    "  cat(sprintf(\"   Test: %s\\n\", test_path))\n",
    "  \n",
    "  # Try to load the files\n",
    "  if (file.exists(train_path) && file.exists(test_path)) {\n",
    "    train_data_raw <- read_csv(train_path, show_col_types = FALSE)\n",
    "    test_data_raw <- read_csv(test_path, show_col_types = FALSE)\n",
    "    \n",
    "    cat(\"‚úÖ Successfully loaded competition data!\\n\")\n",
    "    cat(sprintf(\"   Train: %d rows √ó %d columns\\n\", nrow(train_data_raw), ncol(train_data_raw)))\n",
    "    cat(sprintf(\"   Test: %d rows √ó %d columns\\n\", nrow(test_data_raw), ncol(test_data_raw)))\n",
    "    cat(\"   Columns:\", paste(head(names(train_data_raw), 5), collapse = \", \"), \"\\n\")\n",
    "    \n",
    "    # Auto-detect target and ID columns (common patterns)\n",
    "    possible_targets <- c(\"target\", \"label\", \"y\", \"survived\", \"sale_price\", \"price\")\n",
    "    possible_ids <- c(\"id\", \"Id\", \"ID\", \"PassengerId\", \"customer_id\", \"row_id\")\n",
    "    \n",
    "    TARGET_COL <- NULL\n",
    "    ID_COL <- NULL\n",
    "    \n",
    "    # Find target column\n",
    "    for (col in possible_targets) {\n",
    "      if (col %in% names(train_data_raw)) {\n",
    "        TARGET_COL <- col\n",
    "        break\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    # Find ID column  \n",
    "    for (col in possible_ids) {\n",
    "      if (col %in% names(train_data_raw)) {\n",
    "        ID_COL <- col\n",
    "        break\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    # If not found, make educated guesses\n",
    "    if (is.null(TARGET_COL)) {\n",
    "      # Usually the last column or contains specific keywords\n",
    "      last_col <- names(train_data_raw)[ncol(train_data_raw)]\n",
    "      TARGET_COL <- last_col\n",
    "      cat(\"‚ö†Ô∏è  Target column not auto-detected. Using last column:\", TARGET_COL, \"\\n\")\n",
    "    } else {\n",
    "      cat(\"üéØ Auto-detected target column:\", TARGET_COL, \"\\n\")\n",
    "    }\n",
    "    \n",
    "    if (is.null(ID_COL)) {\n",
    "      # Usually the first column\n",
    "      first_col <- names(train_data_raw)[1]\n",
    "      ID_COL <- first_col  \n",
    "      cat(\"‚ö†Ô∏è  ID column not auto-detected. Using first column:\", ID_COL, \"\\n\")\n",
    "    } else {\n",
    "      cat(\"üÜî Auto-detected ID column:\", ID_COL, \"\\n\")\n",
    "    }\n",
    "    \n",
    "    # Prepare training data\n",
    "    your_train_data_frame <- train_data_raw %>%\n",
    "      mutate(\n",
    "        # Convert target to factor (handle both numeric and character)\n",
    "        !!sym(TARGET_COL) := factor(!!sym(TARGET_COL))\n",
    "      ) %>%\n",
    "      rename(target_variable = !!sym(TARGET_COL))\n",
    "    \n",
    "    # Store test data\n",
    "    test_data_processed <- test_data_raw\n",
    "    \n",
    "    KAGGLE_MODE <- TRUE\n",
    "    \n",
    "  } else {\n",
    "    cat(\"‚ùå Competition files not found, falling back to demo data\\n\")\n",
    "    KAGGLE_MODE <- FALSE\n",
    "  }\n",
    "} else {\n",
    "  cat(\"üìù No datasets found or not in Kaggle environment, using demo data\\n\")\n",
    "  KAGGLE_MODE <- FALSE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
    "execution": {
     "iopub.execute_input": "2025-10-31T20:02:37.071328Z",
     "iopub.status.busy": "2025-10-31T20:02:37.068995Z",
     "iopub.status.idle": "2025-10-31T20:02:41.579361Z",
     "shell.execute_reply": "2025-10-31T20:02:41.571014Z"
    },
    "trusted": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚îÄ‚îÄ \u001b[1mAttaching packages\u001b[22m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels 1.2.0 ‚îÄ‚îÄ\n",
      "\n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mbroom       \u001b[39m 1.0.6      \u001b[32m‚úî\u001b[39m \u001b[34mrecipes     \u001b[39m 1.0.10\n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mdials       \u001b[39m 1.2.1      \u001b[32m‚úî\u001b[39m \u001b[34mrsample     \u001b[39m 1.2.1 \n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mdplyr       \u001b[39m 1.1.4      \u001b[32m‚úî\u001b[39m \u001b[34mtibble      \u001b[39m 3.2.1 \n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mggplot2     \u001b[39m 3.5.1      \u001b[32m‚úî\u001b[39m \u001b[34mtidyr       \u001b[39m 1.3.1 \n",
      "\u001b[32m‚úî\u001b[39m \u001b[34minfer       \u001b[39m 1.0.7      \u001b[32m‚úî\u001b[39m \u001b[34mtune        \u001b[39m 1.2.1 \n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mmodeldata   \u001b[39m 1.4.0      \u001b[32m‚úî\u001b[39m \u001b[34mworkflows   \u001b[39m 1.1.4 \n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mparsnip     \u001b[39m 1.2.1      \u001b[32m‚úî\u001b[39m \u001b[34mworkflowsets\u001b[39m 1.1.0 \n",
      "\u001b[32m‚úî\u001b[39m \u001b[34mpurrr       \u001b[39m 1.0.2      \u001b[32m‚úî\u001b[39m \u001b[34myardstick   \u001b[39m 1.3.1 \n",
      "\n",
      "‚îÄ‚îÄ \u001b[1mConflicts\u001b[22m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidymodels_conflicts() ‚îÄ‚îÄ\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mpurrr\u001b[39m::\u001b[32mdiscard()\u001b[39m masks \u001b[34mscales\u001b[39m::discard()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m  masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m     masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m‚úñ\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mstep()\u001b[39m  masks \u001b[34mstats\u001b[39m::step()\n",
      "\u001b[34m‚Ä¢\u001b[39m Search for functions across packages at \u001b[32mhttps://www.tidymodels.org/find/\u001b[39m\n",
      "\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): object 'your_train_data_frame' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): object 'your_train_data_frame' not found\nTraceback:\n",
      "1. initial_split(data = your_train_data_frame, prop = 0.8, strata = target_variable)",
      "2. mc_cv(data = data, prop = prop, strata = {\n .     {\n .         strata\n .     }\n . }, breaks = breaks, pool = pool, times = 1)",
      "3. tidyselect::vars_select(names(data), !!enquo(strata))",
      "4. eval_select_impl(NULL, .vars, expr(c(!!!dots)), include = .include, \n .     exclude = .exclude, strict = .strict, name_spec = unique_name_spec, \n .     uniquely_named = TRUE, error_call = caller_env())"
     ]
    }
   ],
   "source": [
    "# 1. Load Essential Libraries\n",
    "# tidymodels is a meta-package that loads rsample, recipes, parsnip, tune, etc.\n",
    "library(tidymodels)\n",
    "library(ranger) # Engine for a fast Random Forest implementation\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set.seed(42)\n",
    "\n",
    "# ===============================================================================\n",
    "# Data: Smart Mode - Competition Data or Demo Fallback\n",
    "# ===============================================================================\n",
    "# Use competition data if loaded successfully, otherwise fall back to demo data\n",
    "\n",
    "if (!exists(\"KAGGLE_MODE\") || !KAGGLE_MODE || !exists(\"your_train_data_frame\")) {\n",
    "  # FALLBACK: Create a binary classification example from iris\n",
    "  cat(\"üå∏ Using iris demo data (fallback mode)\\n\")\n",
    "  data(iris)\n",
    "  df <- iris\n",
    "  # Convert Species to a binary target: setosa vs other\n",
    "  df$target_variable <- ifelse(df$Species == \"setosa\", \"setosa\", \"other\")\n",
    "  df$target_variable <- factor(df$target_variable, levels = c(\"other\", \"setosa\"))\n",
    "  # Remove original Species column (so recipe uses numeric predictors only)\n",
    "  df$Species <- NULL\n",
    "  your_train_data_frame <- df\n",
    "  rm(df)\n",
    "  \n",
    "  # Create demo test data (remove some rows from training)\n",
    "  set.seed(999)\n",
    "  demo_indices <- sample(nrow(your_train_data_frame), 20)\n",
    "  test_data_processed <- your_train_data_frame[demo_indices, ] %>% select(-target_variable)\n",
    "  your_train_data_frame <- your_train_data_frame[-demo_indices, ]\n",
    "  \n",
    "  TARGET_COL <- \"target_variable\"\n",
    "  ID_COL <- \"row_id\"\n",
    "  \n",
    "  message(\"üìä Demo mode active: Using iris dataset with 130 training samples and 20 test samples\")\n",
    "} else {\n",
    "  message(\"üèÜ Competition mode active: Using loaded Kaggle competition data\")\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Data Split (Training and Validation)\n",
    "# ==============================================================================\n",
    "# Create a stratified split (important for classification to keep target ratios)\n",
    "# Use 80% for training and 20% for local validation\n",
    "# The `strata` argument expects a column name (unquoted) that exists in the data.\n",
    "\n",
    "# Validate that the target exists and is a factor\n",
    "if (!\"target_variable\" %in% names(your_train_data_frame)) {\n",
    "  stop(\"'your_train_data_frame' must contain a column named 'target_variable'.\")\n",
    "}\n",
    "if (!is.factor(your_train_data_frame$target_variable)) {\n",
    "  your_train_data_frame$target_variable <- factor(your_train_data_frame$target_variable)\n",
    "  message(\"Coerced 'target_variable' to a factor.\")\n",
    "}\n",
    "\n",
    "data_split <- initial_split(\n",
    "  data = your_train_data_frame,\n",
    "  prop = 0.80,\n",
    "  strata = target_variable\n",
    ")\n",
    "\n",
    "# Extract the training and validation (test) sets\n",
    "train_data <- training(data_split)\n",
    "val_data <- testing(data_split)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Define Preprocessing/Feature Engineering (Recipe)\n",
    "# ==============================================================================\n",
    "# Create a recipe to define your preprocessing steps\n",
    "# The formula uses target_variable as the outcome. All other columns are predictors.\n",
    "\n",
    "your_recipe <-\n",
    "  recipe(target_variable ~ ., data = train_data) %>%\n",
    "  # Impute missing numeric data with the mean\n",
    "  step_impute_mean(all_numeric_predictors()) %>%\n",
    "  # One-hot encode all nominal (factor/character) predictors\n",
    "  step_dummy(all_nominal_predictors(), -all_outcomes()) %>%\n",
    "  # Remove variables that are all zero or near zero variance\n",
    "  step_nzv(all_predictors()) %>%\n",
    "  # Normalize (center and scale) all numeric data\n",
    "  step_normalize(all_numeric_predictors())\n",
    "\n",
    "# You can inspect the recipe with summary(your_recipe)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Define the Model (fixed hyperparameters so we can fit)\n",
    "# ==============================================================================\n",
    "# To avoid errors from tune() placeholders, we compute a sensible default for mtry\n",
    "# based on the number of predictors in the training set and set min_n to a default.\n",
    "\n",
    "num_predictors <- ncol(select(train_data, -target_variable))\n",
    "mtry_val <- max(1, floor(sqrt(num_predictors)))\n",
    "\n",
    "rf_model <-\n",
    "  rand_forest(\n",
    "    mode = \"classification\",\n",
    "    mtry = mtry_val,\n",
    "    trees = 1000,\n",
    "    min_n = 5\n",
    "  ) %>%\n",
    "  set_engine(\"ranger\", importance = \"impurity\", seed = 42)\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Create the Workflow and Train the Model\n",
    "# ==============================================================================\n",
    "# Bundle the recipe and the model together\n",
    "rf_workflow <- workflow() %>%\n",
    "  add_recipe(your_recipe) %>%\n",
    "  add_model(rf_model)\n",
    "\n",
    "# Fit the workflow to the training data\n",
    "final_model_fit <- rf_workflow %>%\n",
    "  fit(data = train_data)\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. Prediction and Evaluation (on Validation Set)\n",
    "# ==============================================================================\n",
    "# Make predictions on the local validation data. We ask for class probabilities.\n",
    "val_predictions <-\n",
    "  final_model_fit %>%\n",
    "  predict(new_data = val_data, type = \"prob\") %>% # Get probabilities\n",
    "  bind_cols(final_model_fit %>% predict(new_data = val_data, type = \"class\")) %>%\n",
    "  bind_cols(val_data %>% select(target_variable))\n",
    "\n",
    "# The probability column will be named `.pred_<level>`; for the example we created\n",
    "# this will be `.pred_setosa`. Replace `.pred_setosa` below with the name of the\n",
    "# positive-class probability in your run if you changed class names.\n",
    "prob_col <- grep(\"^\\\\.pred_\", names(val_predictions), value = TRUE)\n",
    "prob_col\n",
    "\n",
    "# Show a quick head of predictions\n",
    "print(head(val_predictions))\n",
    "\n",
    "# Metrics: accuracy and ROC AUC (binary only)\n",
    "# For ROC AUC we explicitly set event_level = \"second\" because the positive class\n",
    "# in this notebook's example is the second level of the factor (\"setosa\").\n",
    "metric_set <- metric_set(accuracy, roc_auc)\n",
    "\n",
    "# Identify the positive class probability column (e.g. .pred_setosa)\n",
    "pos_prob_name <- prob_col[1]\n",
    "\n",
    "# Compute accuracy (uses the predicted class column .pred_class)\n",
    "acc <- accuracy(val_predictions, truth = target_variable, estimate = .pred_class)\n",
    "print(acc)\n",
    "\n",
    "# Compute ROC AUC only if we have a binary problem\n",
    "if (nlevels(your_train_data_frame$target_variable) == 2) {\n",
    "  # Use `!!sym(pos_prob_name)` to pass the probability column to roc_auc\n",
    "  roc_res <- roc_auc(val_predictions, truth = target_variable, !!rlang::sym(pos_prob_name), event_level = \"second\")\n",
    "  print(roc_res)\n",
    "} else {\n",
    "  message(\"ROC AUC skipped: target has more than 2 levels. For multiclass use `roc_auc_multiclass()` or other multiclass metrics.\")\n",
    "}\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat_res <- conf_mat(val_predictions, truth = target_variable, estimate = .pred_class)\n",
    "print(conf_mat_res)\n",
    "\n",
    "# ==============================================================================\n",
    "# Notes:\n",
    "# - Replace 'your_train_data_frame' in your environment with your real dataset.\n",
    "# - Ensure the dataset contains a factor column named 'target_variable'.\n",
    "# - If you want to tune hyperparameters (mtry, min_n) use `tune_grid()` and resampling,\n",
    "#   but remove `tune()` placeholders before fitting directly.\n",
    "# - For multiclass problems, change evaluation metrics accordingly.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. HYPERPARAMETER TUNING (Production-Ready)\n",
    "# ==============================================================================\n",
    "# The previous section used fixed hyperparameters for a quick demo.\n",
    "# This section implements proper cross-validation tuning for optimal performance.\n",
    "\n",
    "# Check dependencies from previous cells\n",
    "if (!exists(\"train_data\") || !exists(\"val_data\") || !exists(\"your_recipe\")) {\n",
    "  stop(\"‚ùå Missing required objects! Please run the previous cells first:\\n\",\n",
    "       \"   - Cell 4: Creates train_data, val_data, and your_recipe\\n\",\n",
    "       \"   - Make sure all previous cells completed successfully\")\n",
    "}\n",
    "\n",
    "cat(\"‚úÖ Dependencies check passed - proceeding with hyperparameter tuning\\n\")\n",
    "\n",
    "library(tune)\n",
    "library(dials)\n",
    "\n",
    "# 7.1 Create Cross-Validation Folds\n",
    "set.seed(123)\n",
    "cv_folds <- vfold_cv(\n",
    "  data = train_data, \n",
    "  v = 10,                    # 10-fold cross-validation\n",
    "  strata = target_variable   # Maintain class balance across folds\n",
    ")\n",
    "\n",
    "print(paste(\"Created\", nrow(cv_folds), \"cross-validation folds\"))\n",
    "\n",
    "# 7.2 Define Tunable Model Specification\n",
    "rf_tuned_spec <- \n",
    "  rand_forest(\n",
    "    mode = \"classification\",\n",
    "    mtry = tune(),           # Number of variables at each split\n",
    "    trees = 1000,            # Keep trees fixed (1000 is usually sufficient)\n",
    "    min_n = tune()           # Minimum samples per leaf node\n",
    "  ) %>%\n",
    "  set_engine(\"ranger\", \n",
    "             importance = \"impurity\",\n",
    "             seed = 42)\n",
    "\n",
    "# 7.3 Create Tuning Workflow\n",
    "rf_tuned_workflow <- workflow() %>%\n",
    "  add_recipe(your_recipe) %>%\n",
    "  add_model(rf_tuned_spec)\n",
    "\n",
    "# 7.4 Define Hyperparameter Grid\n",
    "# Create a reasonable search space\n",
    "num_features <- ncol(select(train_data, -target_variable))\n",
    "\n",
    "tuning_grid <- grid_regular(\n",
    "  mtry(range = c(2, min(10, num_features))),  # 2 to 10 features (or max available)\n",
    "  min_n(range = c(2, 20)),                    # 2 to 20 minimum samples per node\n",
    "  levels = 5                                  # 5x5 = 25 combinations\n",
    ")\n",
    "\n",
    "print(paste(\"Created tuning grid with\", nrow(tuning_grid), \"parameter combinations\"))\n",
    "head(tuning_grid)\n",
    "\n",
    "# 7.5 Execute Hyperparameter Tuning\n",
    "print(\"Starting hyperparameter tuning... This may take a few minutes.\")\n",
    "\n",
    "tuning_results <- \n",
    "  rf_tuned_workflow %>%\n",
    "  tune_grid(\n",
    "    resamples = cv_folds,\n",
    "    grid = tuning_grid,\n",
    "    metrics = metric_set(roc_auc, accuracy, sens, spec),\n",
    "    control = control_grid(save_pred = TRUE, verbose = TRUE)\n",
    "  )\n",
    "\n",
    "print(\"Hyperparameter tuning completed!\")\n",
    "\n",
    "# 7.6 Examine Tuning Results\n",
    "collect_metrics(tuning_results) %>%\n",
    "  filter(.metric == \"roc_auc\") %>%\n",
    "  arrange(desc(mean)) %>%\n",
    "  head(10)\n",
    "\n",
    "# 7.7 Select Best Parameters and Finalize Workflow\n",
    "best_params <- select_best(tuning_results, metric = \"roc_auc\")\n",
    "print(\"Best hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Finalize the workflow with best parameters\n",
    "final_tuned_workflow <- finalize_workflow(rf_tuned_workflow, best_params)\n",
    "\n",
    "# 7.8 Train Final Model on Full Training Set\n",
    "print(\"Training final model with optimized hyperparameters...\")\n",
    "final_tuned_fit <- final_tuned_workflow %>%\n",
    "  fit(data = train_data)\n",
    "\n",
    "print(\"Final model training completed!\")\n",
    "\n",
    "# 7.9 Compare: Tuned vs Untuned Performance on Validation Set\n",
    "tuned_predictions <- \n",
    "  final_tuned_fit %>%\n",
    "  predict(new_data = val_data, type = \"prob\") %>%\n",
    "  bind_cols(final_tuned_fit %>% predict(new_data = val_data, type = \"class\")) %>%\n",
    "  bind_cols(val_data %>% select(target_variable))\n",
    "\n",
    "# Compute metrics for comparison\n",
    "untuned_roc <- roc_auc(val_predictions, truth = target_variable, \n",
    "                       !!rlang::sym(names(val_predictions)[1]))\n",
    "tuned_roc <- roc_auc(tuned_predictions, truth = target_variable, \n",
    "                     !!rlang::sym(names(tuned_predictions)[1]))\n",
    "\n",
    "untuned_acc <- accuracy(val_predictions, truth = target_variable, estimate = .pred_class)\n",
    "tuned_acc <- accuracy(tuned_predictions, truth = target_variable, estimate = .pred_class)\n",
    "\n",
    "cat(\"\\n=== PERFORMANCE COMPARISON ===\\n\")\n",
    "cat(\"Untuned Model:\\n\")\n",
    "cat(\"  ROC AUC:\", round(untuned_roc$.estimate, 4), \"\\n\")\n",
    "cat(\"  Accuracy:\", round(untuned_acc$.estimate, 4), \"\\n\")\n",
    "cat(\"Tuned Model:\\n\") \n",
    "cat(\"  ROC AUC:\", round(tuned_roc$.estimate, 4), \"\\n\")\n",
    "cat(\"  Accuracy:\", round(tuned_acc$.estimate, 4), \"\\n\")\n",
    "cat(\"Improvement:\\n\")\n",
    "cat(\"  ROC AUC:\", sprintf(\"%+.4f\", tuned_roc$.estimate - untuned_roc$.estimate), \"\\n\")\n",
    "cat(\"  Accuracy:\", sprintf(\"%+.4f\", tuned_acc$.estimate - untuned_acc$.estimate), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 8. FEATURE IMPORTANCE ANALYSIS\n",
    "# ==============================================================================\n",
    "# Extract and visualize feature importance from the trained Random Forest model\n",
    "\n",
    "# Check dependencies from previous cells\n",
    "if (!exists(\"final_tuned_fit\")) {\n",
    "  stop(\"‚ùå Missing 'final_tuned_fit' object! Please run cell 5 (Hyperparameter Tuning) first.\\n\",\n",
    "       \"   This cell creates the tuned model needed for feature importance analysis.\")\n",
    "}\n",
    "\n",
    "cat(\"‚úÖ Tuned model found - proceeding with feature importance analysis\\n\")\n",
    "\n",
    "library(vip)      # For variable importance plots\n",
    "library(ggplot2)  # For enhanced plotting\n",
    "\n",
    "# 8.1 Extract Feature Importance\n",
    "# The ranger engine calculates importance when importance = \"impurity\" is set\n",
    "feature_importance <- final_tuned_fit %>%\n",
    "  extract_fit_parsnip() %>%\n",
    "  vi()\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(head(feature_importance, 10))\n",
    "\n",
    "# 8.2 Create Feature Importance Visualization\n",
    "importance_plot <- feature_importance %>%\n",
    "  slice_head(n = min(15, nrow(feature_importance))) %>%  # Top 15 or all if fewer\n",
    "  mutate(Variable = reorder(Variable, Importance)) %>%\n",
    "  ggplot(aes(x = Importance, y = Variable)) +\n",
    "  geom_col(fill = \"steelblue\", alpha = 0.8) +\n",
    "  geom_text(aes(label = round(Importance, 2)), \n",
    "            hjust = -0.1, size = 3) +\n",
    "  labs(\n",
    "    title = \"Random Forest Feature Importance\",\n",
    "    subtitle = \"Top predictive features (Gini impurity reduction)\",\n",
    "    x = \"Importance Score\",\n",
    "    y = \"Features\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(size = 14, face = \"bold\"),\n",
    "    plot.subtitle = element_text(size = 12),\n",
    "    axis.text = element_text(size = 10)\n",
    "  )\n",
    "\n",
    "print(importance_plot)\n",
    "\n",
    "# 8.3 Feature Importance Summary Statistics\n",
    "cat(\"\\n=== FEATURE IMPORTANCE SUMMARY ===\\n\")\n",
    "cat(\"Total features:\", nrow(feature_importance), \"\\n\")\n",
    "cat(\"Top feature:\", feature_importance$Variable[1], \n",
    "    \"(Importance:\", round(feature_importance$Importance[1], 2), \")\\n\")\n",
    "cat(\"Mean importance:\", round(mean(feature_importance$Importance), 2), \"\\n\")\n",
    "cat(\"Features with >50% of max importance:\", \n",
    "    sum(feature_importance$Importance > 0.5 * max(feature_importance$Importance)), \"\\n\")\n",
    "\n",
    "# 8.4 Alternative: Use vip package for cleaner visualization\n",
    "vip_plot <- final_tuned_fit %>%\n",
    "  extract_fit_parsnip() %>%\n",
    "  vip(num_features = min(15, nrow(feature_importance)),\n",
    "      geom = \"col\",\n",
    "      aesthetics = list(fill = \"darkorange\", alpha = 0.8)) +\n",
    "  labs(\n",
    "    title = \"Variable Importance Plot\",\n",
    "    subtitle = \"Alternative visualization using vip package\"\n",
    "  ) +\n",
    "  theme_minimal()\n",
    "\n",
    "print(vip_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 9. TEST SET PREDICTIONS & KAGGLE SUBMISSION\n",
    "# ==============================================================================\n",
    "# Generate predictions for test set and create submission file\n",
    "\n",
    "# Check dependencies from previous cells\n",
    "if (!exists(\"final_tuned_fit\") || !exists(\"test_data_processed\")) {\n",
    "  stop(\"‚ùå Missing required objects! Please run previous cells first:\\n\",\n",
    "       \"   - Cell 3: Creates test_data_processed\\n\", \n",
    "       \"   - Cell 5: Creates final_tuned_fit (tuned model)\\n\",\n",
    "       \"   Make sure all previous cells completed successfully\")\n",
    "}\n",
    "\n",
    "cat(\"‚úÖ All dependencies found - proceeding with test predictions\\n\")\n",
    "\n",
    "# 9.1 Load and Prepare Test Data (Kaggle Competition Mode - ACTIVE)\n",
    "# Production mode: Using actual competition test data\n",
    "\n",
    "test_data_processed <- test_data_processed %>%\n",
    "  # Apply the same preprocessing as training data (outside of recipe)\n",
    "  # Add any custom feature engineering here that matches training data\n",
    "  mutate(\n",
    "    # Example transformations (match your training data preprocessing)\n",
    "    # Add any feature engineering that was applied to training data\n",
    "    # new_feature = some_transformation(existing_feature)\n",
    "  )\n",
    "\n",
    "# 9.2 DEVELOPMENT MODE (commented out for production)\n",
    "# demo_test_data <- val_data %>% \n",
    "#   select(-target_variable)\n",
    "# cat(\"Demo test set created with\", nrow(demo_test_data), \"samples and\", \n",
    "#     ncol(demo_test_data), \"features\\n\")\n",
    "\n",
    "# Production: Use actual test data\n",
    "demo_test_data <- test_data_processed\n",
    "cat(\"Production test set loaded with\", nrow(demo_test_data), \"samples and\", \n",
    "    ncol(demo_test_data), \"features\\n\")\n",
    "\n",
    "# 9.3 Generate Test Predictions\n",
    "print(\"Generating test set predictions...\")\n",
    "\n",
    "test_predictions <- final_tuned_fit %>%\n",
    "  predict(new_data = demo_test_data, type = \"prob\") %>%\n",
    "  bind_cols(final_tuned_fit %>% predict(new_data = demo_test_data, type = \"class\"))\n",
    "\n",
    "# Add row IDs (in real Kaggle competition, use the actual ID column)\n",
    "test_predictions <- test_predictions %>%\n",
    "  mutate(id = row_number()) %>%\n",
    "  select(id, everything())\n",
    "\n",
    "print(\"Test predictions generated successfully!\")\n",
    "head(test_predictions)\n",
    "\n",
    "# 9.4 Create Kaggle Submission File (Production Mode)\n",
    "# Use actual ID column from test data and appropriate prediction format\n",
    "\n",
    "# Extract actual IDs from test data (use the ID_COL defined earlier)\n",
    "actual_ids <- test_data_raw[[ID_COL]]\n",
    "\n",
    "# For binary classification, typically submit probabilities of positive class\n",
    "submission_data <- test_predictions %>%\n",
    "  mutate(!!sym(ID_COL) := actual_ids) %>%\n",
    "  select(\n",
    "    !!sym(ID_COL),                                # Use actual ID column name\n",
    "    # For binary: select probability of positive class (level 2)\n",
    "    prediction = 2                                # This selects the 2nd probability column\n",
    "  )\n",
    "\n",
    "# Alternative: if competition wants class predictions instead of probabilities\n",
    "submission_classes <- test_predictions %>%\n",
    "  mutate(!!sym(ID_COL) := actual_ids) %>%\n",
    "  select(\n",
    "    !!sym(ID_COL),\n",
    "    prediction = .pred_class\n",
    "  ) %>%\n",
    "  mutate(\n",
    "    # Convert factor to numeric if needed (0/1 instead of factor levels)\n",
    "    prediction = as.numeric(prediction) - 1\n",
    "  )\n",
    "\n",
    "# 9.5 Write Submission Files\n",
    "write.csv(submission_data, \"submission_probabilities.csv\", row.names = FALSE)\n",
    "write.csv(submission_classes, \"submission_classes.csv\", row.names = FALSE)\n",
    "\n",
    "cat(\"\\n=== SUBMISSION FILES CREATED ===\\n\")\n",
    "cat(\"üìÅ submission_probabilities.csv - Probability predictions\\n\")\n",
    "cat(\"üìÅ submission_classes.csv - Class predictions (0/1)\\n\")\n",
    "cat(\"Choose the appropriate file based on competition requirements.\\n\")\n",
    "\n",
    "# 9.6 Submission File Preview\n",
    "cat(\"\\nSubmission file preview (probabilities):\\n\")\n",
    "print(head(submission_data))\n",
    "\n",
    "cat(\"\\nSubmission file preview (classes):\\n\") \n",
    "print(head(submission_classes))\n",
    "\n",
    "# 9.7 Final Model Summary for Documentation\n",
    "cat(\"\\n=== FINAL MODEL SUMMARY ===\\n\")\n",
    "cat(\"Model Type: Random Forest (ranger engine)\\n\")\n",
    "cat(\"Tuned Parameters:\\n\")\n",
    "cat(\"  - mtry:\", best_params$mtry, \"\\n\")\n",
    "cat(\"  - min_n:\", best_params$min_n, \"\\n\")\n",
    "cat(\"  - trees: 1000 (fixed)\\n\")\n",
    "cat(\"Cross-Validation Performance (ROC AUC):\", \n",
    "    round(tuned_roc$.estimate, 4), \"\\n\")\n",
    "cat(\"Features used:\", nrow(feature_importance), \"\\n\")\n",
    "cat(\"Training samples:\", nrow(train_data), \"\\n\")\n",
    "cat(\"Validation samples:\", nrow(val_data), \"\\n\")\n",
    "cat(\"Test predictions:\", nrow(test_predictions), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Next Steps & Advanced Techniques\n",
    "\n",
    "### For Higher Kaggle Scores:\n",
    "1. **Ensemble Methods**: Combine Random Forest with XGBoost, LightGBM\n",
    "2. **Advanced Feature Engineering**: Create interaction terms, polynomial features\n",
    "3. **Stacking/Blending**: Use multiple models and meta-learners\n",
    "4. **Hyperparameter Optimization**: Try Bayesian optimization with `tune_bayes()`\n",
    "5. **Cross-Validation Strategies**: Experiment with different CV schemes\n",
    "\n",
    "### Model Diagnostics:\n",
    "- **Learning Curves**: Plot performance vs training set size\n",
    "- **Validation Curves**: Plot performance vs hyperparameter values  \n",
    "- **Residual Analysis**: For regression problems\n",
    "- **ROC Curves**: Detailed threshold analysis\n",
    "\n",
    "### Production Deployment:\n",
    "- **Model Serialization**: Save with `saveRDS()` for later use\n",
    "- **Pipeline Validation**: Test on completely new data\n",
    "- **Monitoring**: Track model performance over time\n",
    "\n",
    "---\n",
    "\n",
    "**üìã Summary**: This notebook provides a complete, production-ready Random Forest pipeline with hyperparameter tuning, evaluation, and submission file generation. Simply uncomment the Kaggle data loading sections and update the file paths to use with your competition data."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1256,
     "sourceId": 2242,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1906,
     "sourceId": 3296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 26658,
     "sourceId": 33974,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2798,
     "sourceId": 7251,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 22685,
     "sourceId": 337535,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 33225,
     "sourceId": 44131,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 500959,
     "sourceId": 2948224,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2069195,
     "sourceId": 11387881,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1237036,
     "sourceId": 2947278,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4728129,
     "sourceId": 8023365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6485000,
     "sourceId": 10473687,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 829456,
     "sourceId": 1432361,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7591428,
     "sourceId": 12061045,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 612177,
     "sourceId": 13502270,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8616374,
     "sourceId": 13571281,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6983063,
     "sourceId": 11187187,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30749,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "r",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
